[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am currently a fourth year at UCLA, majoring in Statistics and Data Science and minoring in Digital Humanities. You can learn more about me through my CV, or you can also check out my Experience and recent Projects."
  },
  {
    "objectID": "index.html#heading-1",
    "href": "index.html#heading-1",
    "title": "",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nAttached is my current CV"
  },
  {
    "objectID": "index.html#heading-2",
    "href": "index.html#heading-2",
    "title": "",
    "section": "Heading 2",
    "text": "Heading 2"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/index.html#heading-1",
    "href": "projects/index.html#heading-1",
    "title": "",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/index.html#heading-2",
    "href": "projects/index.html#heading-2",
    "title": "",
    "section": "Heading 2",
    "text": "Heading 2"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "STATS 140XP: Practice of Statistical Consulting What Makes a Good Company? (Winter 2024)\nSTATS 101C: Intro to Statistical Models and Data Mining IMDb Review Rating Prediction (Fall 2023)\nSTATS 101A: Intro to Data Analysis and Regression Graduate Student Admission Prediction (Winter 2023)\nDGT HUM 150: Machine Learning for Humanists Image Classification of Waste (Winter 2024)\nDGT HUM 131: Digital Mapping and Critical GIS LA Metro: A Spatial Perspective on Its Problems (Fall 2023)\nDGT HUM 101: Intro to Digital Humanities Airbnb in Los Angeles: Short-Term Housing in the Long Run (Fall 2022)\nCOMM 156: Social Networking Passing Networks in Soccer (Winter 2024)"
  },
  {
    "objectID": "projects.html#ucla",
    "href": "projects.html#ucla",
    "title": "",
    "section": "",
    "text": "STATS 101C: Intro to Statistical Models and Data Mining IMDb Review Rating Prediction (Fall 2023)\nDGT HUM 131: Digital Mapping and Critical GIS LA Metro: A Spatial Perspective on Its Problems (Fall 2023)\nSTATS 101A: Intro to Data Analysis and Regression Graduate Student Admission Prediction (Winter 2023)\nDGT HUM 101: Intro to Digital Humanities Airbnb in Los Angeles: Short-Term Housing in the Long Run (Fall 2022)"
  },
  {
    "objectID": "projects.html#datares-at-ucla",
    "href": "projects.html#datares-at-ucla",
    "title": "",
    "section": "DataRes at UCLA",
    "text": "DataRes at UCLA"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Home",
    "section": "",
    "text": "I am currently a fourth year at UCLA, majoring in Statistics and Data Science and minoring in Digital Humanities. You can learn more about me through my CV, or you can also check out my Experience and recent Projects."
  },
  {
    "objectID": "projects.html#datares-ucla",
    "href": "projects.html#datares-ucla",
    "title": "Projects",
    "section": "DataRes @ UCLA",
    "text": "DataRes @ UCLA\nColorless Art: Exploring Museum Diversity (Fall 2023)\nGDP is Outdated: We Need a Better Alternative (Winter 2022)"
  },
  {
    "objectID": "projects.html#bruin-sports-analytics",
    "href": "projects.html#bruin-sports-analytics",
    "title": "Projects",
    "section": "Bruin Sports Analytics",
    "text": "Bruin Sports Analytics\nThe Declining Presence of the Number 10 (Winter 2022)"
  },
  {
    "objectID": "projects.html#ucla-coursework",
    "href": "projects.html#ucla-coursework",
    "title": "Projects",
    "section": "",
    "text": "STATS 140XP: Practice of Statistical Consulting What Makes a Good Company? (Winter 2024)\nSTATS 101C: Intro to Statistical Models and Data Mining IMDb Review Rating Prediction (Fall 2023)\nSTATS 101A: Intro to Data Analysis and Regression Graduate Student Admission Prediction (Winter 2023)\nDGT HUM 150: Machine Learning for Humanists Image Classification of Waste (Winter 2024)\nDGT HUM 131: Digital Mapping and Critical GIS LA Metro: A Spatial Perspective on Its Problems (Fall 2023)\nDGT HUM 101: Intro to Digital Humanities Airbnb in Los Angeles: Short-Term Housing in the Long Run (Fall 2022)\nCOMM 156: Social Networking Passing Networks in Soccer (Winter 2024)"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Modern Endangered Archives Program (UCLA)\nData Scientist Part-Time\nApril 2023 - Present\n\nAnalyzed text data of digitized artifacts from 30+ countries using NLP (e.g. TF-IDF matrix) and machine learning algorithms (e.g. topic modeling) in R. Generated 100+ visualizations to support decision-making for upcoming projects.\n\n\nBruin Sports Analytics\nData Consultant for UCLA Tennis/Football/Baseball\nMarch 2022 - Present\n\nAnalysis Head for the UCLA Tennis team, creating statistical match reports in R and player data dashboards in Tableau.\nCollaborated closely with UCLA Football and Baseball coaching staff to create R Shiny applications features 30+ tables and visualizations. Utilized SQL to query NCAA data for comprehensive game-by-game analysis and player feedback.\n\n\nSimulation Engineering Lab\nData Science Intern\nJuly 2023 - September 2023\n\nDiscovered key insights from housing sales data by building machine learning models for prediction and clustering tasks (e.g. forecasting housing prices with ARIMA) using Python.\n\n\nEduplanner\nData Analysis and Research Intern\nSeptember 2023 - February 2024\n\nAuthored research paper detailing observed impacts on education outcomes following a semester of software utilization, using various machine learning algorithms (e.g. random forest) in R."
  },
  {
    "objectID": "experience.html#experience",
    "href": "experience.html#experience",
    "title": "Experience",
    "section": "",
    "text": "Modern Endangered Archives Program (UCLA)\nData Scientist Part-Time\nApril 2023 - Present\n\nAnalyzed text data of digitized artifacts from 30+ countries using NLP (e.g. TF-IDF matrix) and machine learning algorithms (e.g. topic modeling) in R. Generated 100+ visualizations to support decision-making for upcoming projects.\n\n\nBruin Sports Analytics\nData Consultant for UCLA Tennis/Football/Baseball\nMarch 2022 - Present\n\nAnalysis Head for the UCLA Tennis team, creating statistical match reports in R and player data dashboards in Tableau.\nCollaborated closely with UCLA Football and Baseball coaching staff to create R Shiny applications features 30+ tables and visualizations. Utilized SQL to query NCAA data for comprehensive game-by-game analysis and player feedback.\n\n\nSimulation Engineering Lab\nData Science Intern\nJuly 2023 - September 2023\n\nDiscovered key insights from housing sales data by building machine learning models for prediction and clustering tasks (e.g. forecasting housing prices with ARIMA) using Python.\n\n\nEduplanner\nData Analysis and Research Intern\nSeptember 2023 - February 2024\n\nAuthored research paper detailing observed impacts on education outcomes following a semester of software utilization, using various machine learning algorithms (e.g. random forest) in R."
  },
  {
    "objectID": "docs/Projects/DH_150_Final_Project.html",
    "href": "docs/Projects/DH_150_Final_Project.html",
    "title": "Image Classification of Waste",
    "section": "",
    "text": "Image Classification of Waste\n\nShiyu Murashima\n\n\nMarch 18, 2024\nEfficient waste management hinges on precisely categorizing various types of waste such as plastic, metal, or food organics. Classifying waste is usually done by weight and volume inspection, followed by visual or manual sorting. An acquaintance of mine was engaged in analogous research for Sustainability Action Research at UCLA, in which they classified various types of waste in UCLA trash bins through visual and manual sorting, and conducted an extensive analysis of better waste management. This approach of classifying waste, however, is often hindered by subjectivity, scalability issues, and most importantly, the amount of labor required. This eventually peaked my interest in this topic, as I thought that it could be beneficial to automate this process by using machine learning techniques for image classification. This would offer a more objective and scalable alternative.\nThe goal of this project is to accurately classify images of waste into its correct category. To do this, I will be using the RealWaste dataset from the UCI Machine Learning Repository, collected by Sam Single, Saeid Iranmanesh, and Raad Raad. This dataset consists of color images of waste items taken upon arrival at a landfill site. Each of the images have a resolution of 524x524 pixels. These waste images are also categorized into 9 different labels: cardboard, food organics, glass, metal, miscellaneous trash, paper, plastic, textile trash, and vegetation.\nData: https://archive.ics.uci.edu/dataset/908/realwaste\nCredits: S. Single, S. Iranmanesh, R. Raad, RealWaste, electronic dataset, The UCI Machine Learning Repository, Wollongong City Council, CC BY 4.0\nSam Single, Saeid Iranmanesh, and Raad Raad, who collected this data on waste, created this dataset as apart of an honors thesis researching the performance of convolutional neural networks on authentic waste material when trained on unadulterated objects (separate dataset called DiversionNet), compared to training using real waste materials (RealWaste dataset). The paper focuses solely on the convolutional neural network model, but dives deep into using pre-trained models such as VGG-16, InceptionResNetV2, DenseNet121, InceptionV3, and MobileNetV2 to classify real waste when trained on unadulterated materials, versus when samples are collected at a landfill site. When training on the DiversionNet data, they achieved a classification accuracy of 49.69%, while training on the RealWaste data using the pre-trained InceptionV3 model, they achieved a classification accuracy of 89.19%. With the classification accuracy of the RealWaste data being significantly higher than the DiversionNet data, this consequently revealed the need for sampling training data from a real-life environment to be able to have a classification accuracy greater than a flip of a coin. Therefore, their last suggestion was to consider expanding the RealWaste dataset to account for class imbalance and to train the model with more data.\n\nPreprocessing\nThe RealWaste data was imported via an ImageDataGenerator instance, which helped preprocess and augment each of our images. The ImageDataGenerator rescaled pixel values on a range from 0 to 1, assigned a shear angle of 0.2 degrees in counter-clockwise direction, assigned a zoom range of 0.2, and flipped images horizontally at random. This was then used to read the images into Python, with downscaled image sizes of 128x128 pixels, in batches of 32, with a categorical class mode for multi-class classification.\n\nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define directory containing data\ndataset_dir = 'realwaste-main/RealWaste' # Please change this directory based on where the data is located in your machine\n\n# Define parameters for ImageDataGenerator\nbatch_size = 32\nimage_size = (128, 128)  # Resize images (originally 524x524 px)\n\n# Create an ImageDataGenerator instance for preprocessing and data augmentation\ndatagen = ImageDataGenerator(\n    rescale=1./255,         # Rescale pixel values to [0, 1]\n    shear_range=0.2,        # Shear angle in counter-clockwise direction in degrees\n    zoom_range=0.2,         # Random zoom range\n    horizontal_flip=True    # Randomly flip inputs horizontally\n)\n\n# Create a generator for reading images from the directory\ngenerator = datagen.flow_from_directory(\n    dataset_dir,              \n    target_size=image_size,   \n    batch_size=batch_size,    \n    class_mode='categorical', # 'categorical' for multi-class classification\n    shuffle=False\n)\n\n2024-03-18 16:29:55.516583: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nFound 4752 images belonging to 9 classes.\n\n\nAfter reading in the data, I found that there were 4752 images belonging to the 9 different class labels that were discussed before. Fortunately, there were no missing values to deal with, as the data simply consists of 9 different folders with various counts of images - I will discuss class imbalance shortly after this section. The next step, was to then split the data into training and testing sets; I chose an 80/20 split, following the Pareto principle. This led to a training sample size of 3801 and a testing sample size of 951. Lastly, the data cannot of course just stay as the file names corresponding to the raw images, so I transformed them into numpy arrays with each array consisting of pixel data corresponding to a specific image.\n\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX = generator.filenames\ny = generator.classes\n\n# Splitting data into training and testing sets with 80% training and 20% testing\nX_train_filenames, X_test_filenames, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Extract class labels and filenames from full paths\nX_train_class_labels = [filename.split('/')[0] for filename in X_train_filenames]\nX_train_filenames = [filename.split('/')[-1] for filename in X_train_filenames]\nX_test_class_labels = [filename.split('/')[0] for filename in X_test_filenames]\nX_test_filenames = [filename.split('/')[-1] for filename in X_test_filenames]\n\n# Combine class labels and filenames to create X_train as a list of tuples\nX_train = list(zip(X_train_class_labels, X_train_filenames))\nX_test = list(zip(X_test_class_labels, X_test_filenames))\n\ndef load_and_preprocess_images(file_paths, target_size=(128, 128)):\n    images = []\n    for file_path in file_paths:\n        try:\n            image = cv2.imread(file_path)\n            if image is None:\n                raise FileNotFoundError(f\"Failed to load image: {file_path}\")\n            \n            image = cv2.resize(image, target_size)\n            \n            # Remove channel dimension if present\n            if len(image.shape) == 3:\n                image = np.mean(image, axis=2)\n            \n            # Append preprocessed image to list\n            images.append(image)\n        except Exception as e:\n            print(f\"Error processing image {file_path}: {e}\")\n    images_array = np.array(images) # Converting to numpy array\n    return images_array\n\nX_train = np.array([load_and_preprocess_images([os.path.join(dataset_dir, class_label, filename)]) for class_label, filename in X_train])\nX_test = np.array([load_and_preprocess_images([os.path.join(dataset_dir, class_label, filename)]) for class_label, filename in X_test])\n\n# Fixing shape error\nX_train = np.squeeze(X_train, axis=1)\nX_test = np.squeeze(X_test, axis=1)\n\nprint(\"Training samples:\", len(X_train))\nprint(\"Testing samples:\", len(X_test))\nprint(\"Training shape:\", X_train.shape)\nprint(\"Testing shape:\", X_test.shape)\n\nTraining samples: 3801\nTesting samples: 951\nTraining shape: (3801, 128, 128)\nTesting shape: (951, 128, 128)\n\n\n\n\nBrief Data Exploration\nBefore jumping into the statistical modeling, I wanted to explore the data briefly, to see what kind of images we are dealing with, and to see if there was any class imbalance in the data. The following function simply takes a sample preprocessed image from each class. We can see items like the tube of a roll of paper towels for the cardboard class, a bottle of alcohol for the glass class, an aluminum can for the metal class, a Pinocchio poster for the paper class, or a palm tree for the vegetation class.\n\nimport matplotlib.pyplot as plt\n\n# Function to plot one image from each class\ndef plot_one_image_per_class(generator):\n    class_labels = list(generator.class_indices.keys())\n    \n    # Create a dictionary to store one image from each class\n    one_image_per_class = {label: None for label in class_labels}\n    \n    # Get a batch of data from the generator\n    for _ in range(len(generator)):\n        batch = generator.next()\n        X_batch, y_batch = batch\n        for i in range(len(X_batch)):\n            label = np.argmax(y_batch[i])\n            if one_image_per_class[class_labels[label]] is None:\n                one_image_per_class[class_labels[label]] = X_batch[i]\n    \n    # Plot one image from each class\n    for label, image in one_image_per_class.items():\n        print(f\"Class: {label}\")\n        plt.imshow(image)\n        plt.axis('off')\n        plt.show()\n\nplot_one_image_per_class(generator)\n\nClass: Cardboard\nClass: Food Organics\nClass: Glass\nClass: Metal\nClass: Miscellaneous Trash\nClass: Paper\nClass: Plastic\nClass: Textile Trash\nClass: Vegetation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo see if there was any class imbalance in the data, I created a bar plot showing the counts of the images in each class. There is an average of around 400 images in each class, with the exception of the metal class with around 800 images and the plastic class with around 900 images. This suggests that there is imbalance in our data that may potentially lead to bias in our model performance, as the models may prioritize the majority classes and overlook the minority classes. Thus, it is important to keep in mind the imbalance in our data while evaluating the model results.\n\nclass_labels = list(generator.class_indices.keys())\nclass_counts = generator.classes # Getting counts of each class\n\n# Calculate the number of samples for each class\nclass_distribution = {label: np.sum(class_counts == class_labels.index(label)) for label in class_labels}\n\n# Plot the class distribution\nplt.figure(figsize=(10, 6))\nplt.bar(class_distribution.keys(), class_distribution.values())\nplt.xlabel('Classes')\nplt.ylabel('Number of Samples')\nplt.title('Class Distribution')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStatistical Modeling\nFor my image classification tasks, I will be utilizing logistic regression, naive Bayes, random forest, K-nearest neighbors, support vector machine, XGBoost, and most importantly, dense neural networks and convolutional neural networks.\n\n\nLogistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\n\n# Normalizing pixel values\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# Reshaping data for Logistic Regression/Naive Bayes\nX_train_lr_nb = X_train.reshape(-1, 128*128)\nX_test_lr_nb = X_test.reshape(-1, 128*128)\n\n# Fit scaler on the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_lr_nb)\nX_test_scaled = scaler.transform(X_test_lr_nb)\n\n# Logistic Regression with scaled data\nlr = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr.fit(X_train_scaled, y_train)\nlr_predictions = lr.predict(X_test_scaled)\n\nprint(\"Logistic Regression:\")\nprint(classification_report(y_test, lr_predictions))\n\nLogistic Regression:\n              precision    recall  f1-score   support\n\n           0       0.26      0.30      0.28        92\n           1       0.34      0.16      0.22        82\n           2       0.27      0.58      0.37        84\n           3       0.37      0.29      0.32       158\n           4       0.14      0.11      0.12        99\n           5       0.38      0.42      0.40       100\n           6       0.43      0.50      0.46       185\n           7       0.13      0.11      0.12        64\n           8       0.34      0.14      0.20        87\n\n    accuracy                           0.32       951\n   macro avg       0.29      0.29      0.28       951\nweighted avg       0.32      0.32      0.30       951\n\n\n\nThe first model fitted for our image classification task is logistic regression. Logistic regression is a linear classification algorithm generally used for binary classification tasks, but in our case, we will be implementing the algorithm as a multi-class classification task. It models the probability that an instance belongs to a particular class using the logistic function. I first normalized the pixel values for X_train and X_test, reshaped them, and fit a StandardScaler to transform the data. I then fit the logistic regression model using the limited-memory BFGS solver with a maximum of 1000 iterations. This achieved an accuracy of 32%, with a weighted average precision score of 0.32 (how many images are correctly classified per class), a recall of 0.32 (how many of each class you find over the whole number of elements in the respective class), and a f1-score of 0.30 (the harmonic mean between precision and recall).\n\n\nCross Validation for Logistic Regression\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Define the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lr', LogisticRegression(penalty = \"none\", max_iter=1000))\n])\n\n# Define the hyperparameters grid\nlr_param_grid = {\n    'lr__C': [0.01, 0.1, 1, 10, 100],\n    'lr__penalty': [None, 'l2']\n}\n\n# Perform grid search\nlr_grid_search = GridSearchCV(pipeline, lr_param_grid, cv=2, error_score='raise') # using cv=2 to optimize runtime\nlr_grid_search.fit(X_train_lr_nb, y_train)\n\n# Get best parameters and model\nbest_lr_params = lr_grid_search.best_params_\nbest_lr_estimator = lr_grid_search.best_estimator_\noptimal_lr_predictions = best_lr_estimator.predict(X_test_lr_nb)\n\nprint(\"Optimal hyperparameters:\", best_lr_params)\nprint(\"Logistic Regression (with optimal hyperparameters):\")\nprint(classification_report(y_test, optimal_lr_predictions))\n\nOptimal hyperparameters: {'lr__C': 0.01, 'lr__penalty': 'l2'}\nLogistic Regression (with optimal hyperparameters):\n              precision    recall  f1-score   support\n\n           0       0.30      0.32      0.31        92\n           1       0.41      0.18      0.25        82\n           2       0.30      0.58      0.40        84\n           3       0.38      0.38      0.38       158\n           4       0.15      0.11      0.13        99\n           5       0.44      0.44      0.44       100\n           6       0.40      0.56      0.47       185\n           7       0.14      0.09      0.11        64\n           8       0.45      0.15      0.22        87\n\n    accuracy                           0.35       951\n   macro avg       0.33      0.31      0.30       951\nweighted avg       0.35      0.35      0.33       951\n\n\n\nI then performed a grid search cross validation on the logistic regression to find the optimal hyperparameters for the model. First, I defined a hyperparameter grid with C values of 0.01, 0.1, 1, 10, and 100, with penalty None and L2. The L1 penalty was excluded from the parameters as the solver I used did not support the penalty, hence the warning suppression. Then, I performed the grid search with only two cross validation folds, in order to optimize the runtime. This was a trade-off between computational cost and the quality of the estimate. In the end, the logistic regression results improved slightly after fitting the optimal hyperparameters. This achieved an accuracy of 35%, with a weighted average precision score of 0.35, a recall of 0.35, and f1-score of 0.33.\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nclass_labels = ['Paper', 'Metal', 'Cardboard', 'Food Organics', 'Glass', 'Vegetation',\n                'Textile Trash', 'Miscellaneous Trash', 'Plastic']\n\nconf_matrix = confusion_matrix(y_test, optimal_lr_predictions) # Get the confusion matrix\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis] # Calculate the percentages\nplt.figure(figsize=(10, 8)) # Create a heatmap\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('Logistic Regression Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nLastly, looking at the confusion matrix, we can see that confusion between classes was most significant with miscellaneous trash, followed by glass and plastic. On the other hand, cardboard performed well, along with textile trash.\n\n\nNaive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train_lr_nb, y_train)\nnb_predictions = nb.predict(X_test_lr_nb)\n\nprint(\"Naive Bayes:\")\nprint(classification_report(y_test, nb_predictions))\n\nNaive Bayes:\n              precision    recall  f1-score   support\n\n           0       0.25      0.36      0.30        92\n           1       0.58      0.22      0.32        82\n           2       0.29      0.60      0.39        84\n           3       0.58      0.39      0.46       158\n           4       0.18      0.14      0.16        99\n           5       0.42      0.47      0.44       100\n           6       0.52      0.39      0.45       185\n           7       0.18      0.20      0.19        64\n           8       0.45      0.56      0.50        87\n\n    accuracy                           0.38       951\n   macro avg       0.38      0.37      0.36       951\nweighted avg       0.41      0.38      0.38       951\n\n\n\nThe second model fitted for our image classification task is naive Bayes. Naive Bayes is a probabilistic classifier based on Bayes’ theorem with the “naive” assumption of independence between features. I utilized GaussianNB for this task, and achieved an accuracy of 38%, with a weighted average precision score of 0.41, recall of 0.38, and f1-score of 0.38.\nAdditionally, naive Bayes does not have any hyperparameters that require tuning, so I did not perform a cross validation for this model.\n\nconf_matrix = confusion_matrix(y_test, nb_predictions)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('Naive Bayes Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the confusion matrix, we can see that for the naive Bayes model, confusion between classes was most prominent with glass, while cardboard performed well again, along with plastic.\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train_lr_nb, y_train)\nrf_predictions = rf.predict(X_test_lr_nb)\n\nprint(\"Random Forest:\")\nprint(classification_report(y_test, rf_predictions))\n\nRandom Forest:\n              precision    recall  f1-score   support\n\n           0       0.57      0.48      0.52        92\n           1       0.57      0.32      0.41        82\n           2       0.65      0.52      0.58        84\n           3       0.52      0.67      0.59       158\n           4       0.39      0.30      0.34        99\n           5       0.73      0.56      0.63       100\n           6       0.52      0.72      0.60       185\n           7       0.62      0.25      0.36        64\n           8       0.61      0.84      0.71        87\n\n    accuracy                           0.56       951\n   macro avg       0.57      0.52      0.53       951\nweighted avg       0.56      0.56      0.54       951\n\n\n\nThe third model fitted is a random forest using the RandomForestClassifier. Random forest is an ensemble learning method that builds multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. This model achieved a much higher accuracy of 54%, compared to the previous two models of logistic regression and naive Bayes. Additionally, the classification report outputted a weighted average precision score of 0.54, recall of 0.54, and f1-score of 0.52.\n\n\nCross Validation for Random Forest\n\n# Define the hyperparameter grid\nparam_grid = {\n    'n_estimators': [100, 200] # using estimators 100 and 200\n}\n\n# Perform grid search\nrf_grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=2, n_jobs=-1) # using cv=2 to optimize runtime\nrf_grid_search.fit(X_train_lr_nb, y_train)\n\nbest_rf_estimator = rf_grid_search.best_estimator_\nbest_params = rf_grid_search.best_params_\noptimal_rf_predictions = best_rf_estimator.predict(X_test_lr_nb)\n\nprint(\"Optimal hyperparameters:\", best_params)\nprint(\"Random Forest (with optimal hyperparameters):\")\nprint(classification_report(y_test, optimal_rf_predictions))\n\nOptimal hyperparameters: {'n_estimators': 200}\nRandom Forest (with optimal hyperparameters):\n              precision    recall  f1-score   support\n\n           0       0.61      0.50      0.55        92\n           1       0.67      0.27      0.38        82\n           2       0.65      0.52      0.58        84\n           3       0.48      0.67      0.56       158\n           4       0.35      0.24      0.29        99\n           5       0.73      0.58      0.65       100\n           6       0.52      0.70      0.60       185\n           7       0.62      0.20      0.31        64\n           8       0.55      0.87      0.68        87\n\n    accuracy                           0.55       951\n   macro avg       0.58      0.51      0.51       951\nweighted avg       0.56      0.55      0.53       951\n\n\n\nSimilar to the logistic regression, I then performed a grid search cross validation on the random forest model. First, I defined a hyperparameter grid using 100 and 200 estimators. Then, I performed the grid search using two cross validation folds and -1 number of jobs to again, optimize the run time. The results unfortunately did not change much from the initial model, likely due to the smaller number of estimators and cross validation folds. Nonetheless, this model achieved an accuracy of 55% (1% better than before), a weighted average precision score of 0.55, recall of 0.55, and f1-score of 0.53.\n\nconf_matrix = confusion_matrix(y_test, optimal_rf_predictions)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('Random Forest Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the confusion matrix, we can see that confusion between classes was most significant with miscellaneous trash again, followed by glass. On the other hand, plastic performed extremely well.\n\n\nK-Nearest Neighbors\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors=5) # initial model with 5 neighbors\nknn_model.fit(X_train_lr_nb, y_train)\nknn_predictions = knn_model.predict(X_test_lr_nb)\nknn_report = classification_report(y_test, knn_predictions)\n\nprint(\"\\nKNN Classification Report:\")\nprint(knn_report)\n\n\nKNN Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.33      0.26      0.29        92\n           1       0.42      0.10      0.16        82\n           2       0.18      0.80      0.29        84\n           3       0.50      0.34      0.40       158\n           4       0.24      0.27      0.26        99\n           5       0.57      0.25      0.35       100\n           6       0.38      0.42      0.40       185\n           7       0.36      0.06      0.11        64\n           8       1.00      0.02      0.04        87\n\n    accuracy                           0.30       951\n   macro avg       0.44      0.28      0.26       951\nweighted avg       0.44      0.30      0.29       951\n\n\n\nThe fourth model fitted is a K-nearest neighbors using the KNeighborsClassifier. K-nearest neighbors is a simple and effective classification algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g. Euclidean distance) to the K-nearest neighbors in the training data. This initial model used 5 neighbors, and achieved an accuracy of 30%, which is slightly worse than the logistic regression, a weighted average precision score of 0.44, recall of 0.30, and f1-score of 0.29.\n\n\nCross Validation for K-Nearest Neighbors\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Define the hyperparameters grid\nknn_param_dist = {\n    'n_neighbors': [3, 5],  # Reduced range to optimize runtime\n    'weights': ['uniform', 'distance'],\n    'p': [1, 2]\n}\n\n# Perform randomized search with cross-validation\nknn_random_search = RandomizedSearchCV(KNeighborsClassifier(), knn_param_dist, n_iter=4, cv=2, random_state=42) # using cv=2 and n_iter=4 to optimize runtime\nknn_random_search.fit(X_train_lr_nb, y_train)\n\n# Get the best parameters and model\nbest_knn_params = knn_random_search.best_params_\nbest_knn_estimator = knn_random_search.best_estimator_\noptimal_knn_predictions = best_knn_estimator.predict(X_test_lr_nb)\n\nprint(\"Optimal hyperparameters:\", best_knn_params)\nprint(\"K-Nearest Neighbors (with optimal hyperparameters):\")\nprint(classification_report(y_test, optimal_knn_predictions))\n\nOptimal hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 3}\nK-Nearest Neighbors (with optimal hyperparameters):\n              precision    recall  f1-score   support\n\n           0       0.50      0.46      0.48        92\n           1       0.36      0.05      0.09        82\n           2       0.25      0.77      0.38        84\n           3       0.56      0.47      0.52       158\n           4       0.25      0.30      0.27        99\n           5       0.60      0.55      0.58       100\n           6       0.45      0.52      0.48       185\n           7       0.35      0.17      0.23        64\n           8       1.00      0.06      0.11        87\n\n    accuracy                           0.40       951\n   macro avg       0.48      0.37      0.35       951\nweighted avg       0.49      0.40      0.38       951\n\n\n\nFor my K-nearest neighbors model, I performed a randomized search cross validation, rather than a grid search, to find the optimal hyperparameters for the model. The hyperparameters grid consisted of 3 and 5 neighbors (to optimize runtime), uniform and distance weights, and p values 1 and 2. I then performed the randomized search with 4 iterations and two cross validation folds. This achieved an accuracy of 40% (10% better than the initial), a weighted average precision score of 0.49, recall of 0.40, and f1-score of 0.38.\n\nconf_matrix = confusion_matrix(y_test, optimal_knn_predictions)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('K-Nearest Neighbors Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the confusion matrix, we can see that confusion between classes was very significant with metal, and surprisingly by plastic. On the other hand, cardboard performed well once again.\n\n\nSupport Vector Machine\n\nfrom sklearn.svm import SVC\n\nsvm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_model.fit(X_train_lr_nb, y_train)\nsvm_predictions = svm_model.predict(X_test_lr_nb)\nsvm_report = classification_report(y_test, svm_predictions)\n\nprint(\"SVM Classification Report:\")\nprint(svm_report)\n\nSVM Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.46      0.40      0.43        92\n           1       0.57      0.44      0.50        82\n           2       0.62      0.56      0.59        84\n           3       0.48      0.61      0.54       158\n           4       0.38      0.30      0.34        99\n           5       0.65      0.49      0.56       100\n           6       0.56      0.71      0.62       185\n           7       0.38      0.09      0.15        64\n           8       0.56      0.82      0.67        87\n\n    accuracy                           0.53       951\n   macro avg       0.52      0.49      0.49       951\nweighted avg       0.52      0.53      0.51       951\n\n\n\nThe fifth model fitted is a support vector machine using SVC. Support vector machine is a powerful supervised learning algorithm used for classification. It finds the hyperplane that best separates classes in a high-dimensional space. This model was fitted using a radial basis function kernel which is often used for capturing complex nonlinear relationships, a C value of 1, and a gamma scaled on the number of input features. This achieved an accuracy of 53%, almost as accurate as the random forest, with a weighted average precision score of 0.52, recall of 0.53, and f1-score of 0.51.\n\n# Define the hyperparameters grid\n# svm_param_dist = {\n#     'C': [0.1, 1],  # Reduced range\n#     'gamma': ['scale'],  # Only scale for gamma\n#     'kernel': ['linear', 'rbf']  # Only linear and rbf for kernel\n# }\n\n# Perform randomized search with cross-validation\n# svm_random_search = RandomizedSearchCV(SVC(), svm_param_dist, n_iter=5, cv=2, random_state=42) # using cv=2 and n_iter=5 to optimize runtime\n# svm_random_search.fit(X_train_lr_nb, y_train)\n\n# Get the best parameters and model\n# best_svm_params = svm_random_search.best_params_\n# best_svm_estimator = svm_random_search.best_estimator_\n# optimal_svm_predictions = best_svm_estimator.predict(X_test_lr_nb)\n\n# print(\"Optimal hyperparameters:\", best_svm_params)\n# print(\"Support Vector Machine (with optimal hyperparameters):\")\n# print(classification_report(y_test, optimal_svm_predictions))\n\nI attempted to perform a randomized search cross validation on the support vector machine model, but the runtime was far too long even after tweaking the hyperparameter grid, decreasing the number of iterations, and decreasing the number of cross validation splits. In case you wanted to run the code on a machine with more computational power, I have left my code as a comment.\n\nconf_matrix = confusion_matrix(y_test, svm_predictions)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('Support Vector Machine Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the confusion matrix, we can see that confusion between classes was very significant with miscellaneous trash only. On the other hand, plastic performed very well, followed by textile trash.\n\n\nXGBoost\n\nfrom xgboost import XGBClassifier\n\nxgb_model = XGBClassifier()\nxgb_model.fit(X_train_lr_nb, y_train)\nxgb_predictions = xgb_model.predict(X_test_lr_nb)\nxgb_report = classification_report(y_test, xgb_predictions)\n\nprint(\"\\nXGBoost Classification Report:\")\nprint(xgb_report)\n\n\nXGBoost Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.60      0.54      0.57        92\n           1       0.56      0.41      0.48        82\n           2       0.62      0.57      0.60        84\n           3       0.58      0.70      0.63       158\n           4       0.42      0.27      0.33        99\n           5       0.67      0.57      0.62       100\n           6       0.55      0.68      0.61       185\n           7       0.41      0.27      0.32        64\n           8       0.59      0.79      0.68        87\n\n    accuracy                           0.57       951\n   macro avg       0.55      0.53      0.54       951\nweighted avg       0.56      0.57      0.55       951\n\n\n\nThe sixth model fitted is a XGBoost model using the XGBClassifier. XGBoost (extreme gradient boosting) is an optimized distributed gradient boosting library designed for efficiency, flexibility, and portability. This model achieved an accuracy of 57%, the best accuracy score so far, with a weighted average precision of 0.56, a recall of 0.57, and f1-score of 0.55.\n\n# Define the hyperparameters grid\n# xgb_param_dist = {\n#     'n_estimators': [100, 200],  # Reduced range for number of estimators\n#     'max_depth': [3, 5],  # Reduced range for max depth\n#     'learning_rate': [0.1, 0.01]  # Reduced range for learning rate\n# }\n\n# Perform randomized search with cross-validation\n# xgb_random_search = RandomizedSearchCV(XGBClassifier(), xgb_param_dist, n_iter=5, cv=2, random_state=42) # using cv=2 and n_iter=5 to optimize runtime\n# xgb_random_search.fit(X_train_lr_nb, y_train)\n\n# Get the best parameters and model\n# best_xgb_params = xgb_random_search.best_params_\n# best_xgb_estimator = xgb_random_search.best_estimator_\n# optimal_xgb_predictions = best_xgb_estimator.predict(X_test_lr_nb)\n\n# print(\"Optimal hyperparameters:\", best_xgb_params)\n# print(\"XGBoost (with optimal hyperparameters):\")\n# print(classification_report(y_test, optimal_xgb_predictions))\n\nI attempted to perform a randomized search cross validation on the XGBoost model as well, but the runtime was too long again. In case you wanted to run the code, I have left my code as a comment.\n\nconf_matrix = confusion_matrix(y_test, xgb_predictions)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('XGBoost Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the confusion matrix, we can see that confusion between classes was somewhat prominent with miscellaneous trash and glass, but not as much as the previous models. On the other hand, plastic performed very well once again, followed by food organics and textile trash.\n\n\nOptimizing Hyperparameters for Dense Neural Network\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\ntf.random.set_seed(42)\n\ninitial_epochs = 10\ninitial_batch_size = 64\ninitial_optimizer = 'adam'\n\n# Function for dense neural network\ndef dnn(num_epochs=initial_epochs, batch_size=initial_batch_size, num_dense_layers=1, optimizer=initial_optimizer):\n    model = Sequential() # Create a Sequential model\n    model.add(Flatten(input_shape=(128, 128))) # Flatten layer to convert 2D image data to 1D\n    for _ in range(num_dense_layers):\n        model.add(Dense(80, activation='relu')) # Add Dense layers with ReLU activation\n    model.add(Dense(10, activation='softmax'))  # Add the final Dense layer with softmax activation\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n    return model, history\n\ninitial_dnn_model, initial_history = dnn()\n\n# Plotting accuracy vs epochs\ndef plot_accuracy(history):\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='test')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy vs Epochs')\n    plt.legend()\n    plt.show()\n\n# Initial configuration\nplot_accuracy(initial_history)\n\n# Changing number of epochs\nnum_epochs = 20\nnum_epochs_dnn, num_epochs_history = dnn(num_epochs=num_epochs)\nplot_accuracy(num_epochs_history)\n\n# Changing batch size\nbatch_size = 128\nbatch_size_dnn, batch_size_history = dnn(batch_size=batch_size)\nplot_accuracy(batch_size_history)\n\n# Changing number of dense layers\nnum_dense_layers = 7\nnum_dense_layers_dnn, num_dense_layers_history = dnn(num_dense_layers=num_dense_layers)\nplot_accuracy(num_dense_layers_history)\n\n# Changing the optimizer\noptimizer = 'sgd'\noptimizer_dnn, optimizer_history = dnn(optimizer=optimizer)\nplot_accuracy(optimizer_history)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe seventh model fitted is a dense neural network using a Sequential model. A dense neural network has a basic architecture where every neuron in one layer is connected to every neuron in the next layer. For our dense neural network with many parameters, there is too many possible changes in the architecture to conduct hyperparameter optimization, such as cross validation. Instead, I fine-tuned the number of epochs, batch size, number of dense layers, and the optimizer manually. I first initialized the number of epochs as 10, the batch size as 64, the number of dense layers as 1, and the optimizer as ‘adam’. Additionally I flattened the layers to convert the 2D image data into 1D, and added dense layers using ReLU and softmax activation. Lastly, to compile the model I used the sparse categorical cross entropy loss. To better understand the accuracy of the training and testing data, I created a function that plots the accuracy by the epoch. In the initial configuration, the training and testing accuracy were relatively the same at around 20%. After increasing the number of epochs to 20, both the training and testing accuracy stayed the same but with more fluctuation. After increasing the batch size to 128, the training and testing accuracy seemed to increase to around 25%. If I increase the number of dense layers from 0 to 7, the training and testing accuracies seemed to increase a bit more at around 35%. And lastly, if I change the optimizer to ‘sgd’, both the training and testing accuracy seemed to increase to around 35%. Now, I will implement these optimal features into my final dense neural network. Note: I altered each of the parameters with various different values, but just chose the most insightful ones for my analysis.\n\n\nDense Neural Network\n\n# Optimized dense neural network\ntf.random.set_seed(42)\n\ndense_nn_model = Sequential() # Create a Sequential model\ndense_nn_model.add(Flatten(input_shape=(128, 128)))  # Flatten layer to convert 2D image data to 1D\nfor _ in range(7):\n    dense_nn_model.add(Dense(80, activation='relu')) # Add Dense layers with ReLU activation\ndense_nn_model.add(Dense(10, activation='softmax'))  # Add the final Dense layer with softmax activation\n\ndense_nn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = dense_nn_model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=0)\ndnn_loss, dnn_accuracy = dense_nn_model.evaluate(X_test, y_test)\n\n30/30 [==============================] - 0s 2ms/step - loss: 1.7342 - accuracy: 0.4027\n\n\n\ndnn_pred = np.argmax(dense_nn_model.predict(X_test), axis=1)\nprint(\"Dense Neural Network:\")\nprint(classification_report(y_test, dnn_pred))\n\n30/30 [==============================] - 0s 2ms/step\nDense Neural Network:\n              precision    recall  f1-score   support\n\n           0       0.56      0.43      0.49        92\n           1       0.36      0.11      0.17        82\n           2       0.44      0.57      0.50        84\n           3       0.34      0.66      0.45       158\n           4       0.20      0.03      0.05        99\n           5       0.56      0.37      0.45       100\n           6       0.45      0.49      0.47       185\n           7       0.22      0.11      0.15        64\n           8       0.35      0.52      0.42        87\n\n    accuracy                           0.40       951\n   macro avg       0.39      0.37      0.35       951\nweighted avg       0.40      0.40      0.37       951\n\n\n\n\ntf.random.set_seed(42)\n\nhistory = dense_nn_model.fit(X_train, y_train, epochs=30, batch_size=64, verbose=0, validation_data=(X_test, y_test))\nplot_accuracy(history)\n\n\n\n\n\n\n\n\nMy optimized dense neural network utilizes 30 epochs, a batch size of 64, 7 dense layers, and the ‘adam’ optimizer. This achieved an accuracy of 40%, with a weighted average precision score of 0.40, recall of 0.40, and f1-score of 0.37. On the graph, you can also see that the training accuracy increased to around 60%.\n\nconf_matrix = confusion_matrix(y_test, dnn_pred)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('Dense Neural Network Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the confusion matrix, we can see that confusion between classes was prominent with glass and miscellaneous trash. On the other hand, food organics and cardboard decently well.\n\n\nOptimizing Hyperparameters for Convolutional Neural Network\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n\ntf.random.set_seed(42)\n\ninitial_num_conv_pool_layers = 2\ninitial_num_filters = 32\ninitial_filter_size = (3, 3)\ninitial_pool_size = (2, 2)\n\nX_train_cnn = X_train.reshape(-1, 128, 128, 1)\nX_test_cnn = X_test.reshape(-1, 128, 128, 1)\n\n# Function for convolutional neural network\ndef cnn(num_conv_pool_layers=initial_num_conv_pool_layers, num_filters=initial_num_filters,\n                         filter_size=initial_filter_size, pool_size=initial_pool_size):\n    model = Sequential() # Initialize the Sequential model\n    model.add(Conv2D(num_filters, filter_size, activation='relu', input_shape=(128, 128, 1))) # Add the first Conv2D layer\n    model.add(MaxPooling2D(pool_size=pool_size)) # Add the first MaxPooling2D layer\n    for _ in range(num_conv_pool_layers - 1):\n        model.add(Conv2D(num_filters, filter_size, activation='relu')) # Add subsequent Conv2D and MaxPooling2D layers\n        model.add(MaxPooling2D(pool_size=pool_size))\n    model.add(Flatten()) # Add the Flatten layer\n    model.add(Dense(128, activation='relu')) # Add the first Dense layer\n    model.add(Dense(10, activation='softmax')) # Add the output Dense layer\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(X_train_cnn, y_train, epochs=10, batch_size=64, validation_data=(X_test_cnn, y_test), verbose=0)\n    return model, history\n\ninitial_cnn_model, initial_cnn_history = cnn()\n\n# Initial configuration\nplot_accuracy(initial_cnn_history)\n\n# Changing number of convolutional/pooling layers\nnum_conv_pool_layers = 3\nnum_conv_pool_layers_cnn, num_conv_pool_layers_history = cnn(num_conv_pool_layers=num_conv_pool_layers)\nplot_accuracy(num_conv_pool_layers_history)\n\n# Changing number of filters\nnum_filters = 64\nnum_filters_cnn, num_filters_history = cnn(num_filters=num_filters)\nplot_accuracy(num_filters_history)\n\n# Changing filter size\nfilter_size = (4, 4)\nfilter_size_cnn, filter_size_history = cnn(filter_size=filter_size)\nplot_accuracy(filter_size_history)\n\n# Changing pool size\npool_size = (3, 3)\npool_size_cnn, pool_size_history = cnn(pool_size=pool_size)\nplot_accuracy(pool_size_history)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe eighth and last model fitted is a convolutional neural network using a Sequential model. A convolutional neural network has a specialized deep learning architecture designed for processing structured grids of data, such as images. It uses convolutional layers to automatically and adaptively learn spatial hierarchies of features. Similar to the dense neural network model, our convolutional neural network has many parameters, making it difficult to make changes in the architecture using hyperparameter optimization. Thus, I will again fine-tune the following parameters: number of convolutional or pooling layers, number of filters, filter size, and pool size. First, I initialized the number of pooling layers as 2, the number of filters as 32, the filter size as (3, 3), and the pool size as (2, 2). I flattened the layers and added dense layers using ReLU and softmax activation. To compile the model, I used the sparse categorical cross entropy loss. In my accuracy plot for the initial configuration, the training accuracy was able to reach around 80%, but the testing accuracy stayed at around 50%. After increasing the number of pooling errors, the training accuracy decreased to around 70%, but the testing accuracy stayed at around 50%. If I increase the number of filters to 64, there is not much change in the training or testing accuracy. If I increase the filter size to (4, 4), the training and testing accuracy both seem to slightly decrease. Lastly, if I increase the pool size to (3, 3), the training accuracy seems to decrease to around 70% again, with the testing accuracy staying at around 50%. Now, I will implement the optimal features into my final convolutional neural network. Note: I altered each of the parameters with various different values, but just chose the most insightful ones for my analysis.\n\n\nConvolutional Neural Network\n\n# Optimized convolutional neural network\n\ntf.random.set_seed(42)\n\ncnn_model = Sequential() # Initialize the Sequential model\ncnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1))) # Add the first Conv2D layer\ncnn_model.add(MaxPooling2D((3, 3))) # Add the first MaxPooling2D layer\nfor _ in range(2):\n    cnn_model.add(Conv2D(32, (3, 3), activation='relu')) # Add subsequent Conv2D and MaxPooling2D layers\n    cnn_model.add(MaxPooling2D((3, 3)))\ncnn_model.add(Flatten()) # Add the Flatten layer\ncnn_model.add(Dense(128, activation='relu')) # Add the first Dense layer\ncnn_model.add(Dense(10, activation='softmax')) # Add the output Dense layer\n\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=64, verbose=0)\ncnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test)\n\n30/30 [==============================] - 1s 17ms/step - loss: 1.3864 - accuracy: 0.5037\n\n\n\ncnn_pred = np.argmax(cnn_model.predict(X_test), axis=1)\nprint(\"Convolutional Neural Network:\")\nprint(classification_report(y_test, cnn_pred))\n\n30/30 [==============================] - 1s 16ms/step\nConvolutional Neural Network:\n              precision    recall  f1-score   support\n\n           0       0.51      0.57      0.54        92\n           1       0.58      0.27      0.37        82\n           2       0.77      0.44      0.56        84\n           3       0.62      0.41      0.49       158\n           4       0.33      0.34      0.34        99\n           5       0.45      0.51      0.48       100\n           6       0.52      0.66      0.58       185\n           7       0.41      0.19      0.26        64\n           8       0.47      0.97      0.64        87\n\n    accuracy                           0.50       951\n   macro avg       0.52      0.48      0.47       951\nweighted avg       0.52      0.50      0.49       951\n\n\n\n\ntf.random.set_seed(42)\n\nhistory = cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=64, verbose=0, validation_data=(X_test_cnn, y_test))\nplot_accuracy(history)\n\n\n\n\n\n\n\n\nMy optimized convolutional neural network utilizes 3 pooling layers, 32 filters, a filter size of (3, 3), and a pool size of (3, 3). This achieved an accuracy of 50%, with a weighted average precision score of 0.52, a recall of 0.50, and a f1-score of 0.49, all of which are slightly better than the convolutional neural network. On the graph, you can also see that the training accuracy was around 65%.\n\nconf_matrix = confusion_matrix(y_test, cnn_pred)\nconf_matrix_percent = conf_matrix / np.sum(conf_matrix, axis=1)[:, np.newaxis]\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_percent, annot=True, cmap='Blues', fmt=\".2%\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Labels')\nplt.ylabel('Actual Labels')\nplt.title('Convolutional Neural Network Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\nLooking at the confusion matrix, we can see that confusion between classes was most significant with miscellaneous trash, while plastic performed exceptionally well, followed somewhat by textile trash.\n\n\nConclusion\nBefore jumping into a comparison of the results between the eight models, it is important to note the advantages and disadvantages of each algorithm. Logistic regression is easy to understand and is efficient for binary classification, however since my image classification task is a multi-class classification, this model may struggle with the data. Additionally, logistic regression has limited expressiveness for complex data like images and does not handle non-linear relationships well. Naive Bayes has fast and efficient training and can handle high-dimensional data well, but assumes independence between features which doesn’t necessarily hold true for images and also has limited capability to capture complex relationships. Random forest is robust to overfitting due to ensemble averaging and is effective for handling high-dimensional data and feature interactions, but may require more computational resources for training and is less interpretable compared to simpler models like decision trees. K-nearest neighbors is also easy to understand and has no training phase, but is computationally expensive during inference especially for large datasets, and is sensitive to the choice of distance metric and the number of neighbors. Support vector machine is effective in high-dimensional spaces and is versatile due to the different kernel function (e.g. RBF), however it can be sensitive to the choice of kernel parameters and the training time can be extremely slow for large datasets (could not do cross validation). XGBoost has high predictive accuracy (achieved best accuracy for this model) and handles missing data well, although we did not have any. However, it requires careful tuning of hyperparameters and can be computationally expensive for large datasets (also could not do cross validation). Dense neural networks can learn complex patterns in data including images and has effective feature learning through multiple layers, however it is prone to overfitting especially with limited data and training can be computationally intensive, especially for deep architectures. Convolutional neural networks are highly effective for image classification tasks and automatically learns hierarchical features, but requires large amounts of labeled data for training and can be computationally expensive, especially for deeper architectures, similar to dense neural networks.\nNow to compare our results from each of the models; XGBoost surprisingly performed best achieving an accuracy of 57%, beating out the convolutional neural network which achieved an accuracy of 50%. On the other end of the spectrum, logistic regression performed worst with an accuracy of 35%. XGBoost likely performed best due to its relatively decent performance in each class and because the model is known for its high predictive accuracy. The best performing classes were plastic with 79.31%, food organics with 69.62%, and textile trash with 68.11%, while the worst performing were miscellaneous trash with 26.56% and glass with 27.27%. To add on, this model did not have hyperparameter optimization, so there is still room for improvement. Random forest performed second best with an accuracy of 55%. The overall performance was not as good as XGBoost, but the plastic class performed extremely well at 88.51%. Support vector machine came third with a rather similar overall performance to random forest, with plastic performing extremely well at 81.41%, but miscellaneous trash performing very poorly at 9.38%. Similar to XGBoost, this model also still has room for improvement as its hyperparameters are not optimized. Both support vector machine and random forest are effective in high-dimensional spaces, which may explain their higher accuracies. Convolutional neural network surprisingly came fourth, even after trying to fine tune its hyperparameters, with an accuracy of 50%. Plastic achieved the highest accuracy across all models with a 96.55% accuracy, while miscellaneous trash struggled at around 18.75%. Fifth came the dense neural network, with an overall accuracy of 40%. This model performed decently for various classes like food organics, cardboard, plastic, and textile trash, but performed extremely poor on glass and miscellaneous trash. The convolutional neural network and dense neural network may have not performed as well due to the training being computationally expensive and having to adjust some parameters accordingly to optimize runtime. Sixth came K-nearest neighbors, achieving an accuracy of only 40%. This model did not perform too well, with the exception of the cardboard class having a 77.38% accuracy. Most notable are the plastic and metal accuracies with extremely low 5.75% and 4.88% accuracies respectively, both the lowest between all models. This may also be due to the computational cost of the algorithm, forcing the number of neighbors to be lower to optimize runtime. Naive Bayes came seventh with an accuracy of 38%, also performing not too well overall with cardboard performing best at 59.52% and glass performing worst at 14.14%. This algorithm has limited capability in capturing complex relationships, so it makes sense that it performed poorly on this dataset. Finally, logistic regression came last with an average overall accuracy of 35%, with cardboard performing best at 58.33% and miscellaneous trash performing joint worst at 9.38%. This image classification task was for multi-class classification rather than binary classification, so it makes sense that it comes in last for accuracy.\nThe accuracies of my models are rather poor when compared to the work of Sam Single, Saeid Iranmanesh, and Raad Raad in their honors thesis. Using the RealWaste data, they were able to achieve an accuracy of 89.19% using a convolutional neural network and the pre-trained InflectionV3 model. By using the InflectionV3 model or another pre-trained model, my accuracy for my convolutional neural network may have shot up, especially since it was just the miscellaneous class that was holding back the overall accuracy. However, I have decided to keep my modeling techniques within the scope of the class, and observe the highest accuracy from there.\nBefore closing off, it is essential to note the biases, interpretability, and ethics with respect to the data and algorithms that have been applied to it. One obvious bias is due to the class imbalance. With the plastic class having the most amount of images, there may be bias towards classifying plastic more accurately, especially after considering how the highest accuracies in the confusion matrices were often plastic. Another bias could be the fact that we are not given any information regarding the geographical location in which the images were taken and from where the waste was collected. If the model primarily learns from images of waste in urban areas, it may not perform as well for rural or suburban waste images. One last bias, although there are many more, could be from human subjectivity, coming from how each of the images were labeled into separate classes beforehand, which may impact the accuracy of the trained models. In terms of the interpretability of the data, there should not be too many problems, but in terms of the algorithms, such as the complex convolutional neural networks, they lack interpretability compared to simpler models like logistic regression or random forest, which may affect how one may interpret the results of the model. Lastly, in terms of ethics, an obvious question is regarding data privacy and how the images were collected and shared. Since this dataset includes waste such as paper, it may contain private information that should not be shared to the public without consent. Finally, it is important to consider the broader societal implications of waste classification algorithms, as automated waste sorting algorithms have potential in impacting employment in the future in recycling facilities or waste management factories.\nTo sum everything up, the XGBoost, support vector machine, and random forest models performed better than expected, while the convolutional neural network and dense neural performed worse than expected. XGBoost has high predictive accuracy, and support vector machine and random forest are effective in high-dimensional spaces, which likely explains their better performance compared to the other models tested. On the other hand convolutional neural network and dense neural network likely performed worse than expected due to the consideration of the adjustment of parameters based on optimizing runtime and minimizing computational cost. Nonetheless, in terms of the performance overall, the best algorithms performed only slightly better than flipping a coin, compared to Sam Single, Saeid Iranmanesh, and Raad Raad’s research who achieved nearly 90% accuracy. In the future, if I were to expand on this project to achieve greater classification accuracies, it may be a good idea to try and apply pre-trained models, as well as consider expanding on the RealWaste dataset to first balance the image counts between classes, and then further refine the trained models by feeding in more images."
  }
]